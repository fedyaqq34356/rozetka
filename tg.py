import argparse
import os
import re
import sys
import time
from datetime import datetime

try:
    import cloudscraper
except ImportError:
    print("[–ü–û–ú–ò–õ–ö–ê] –ü–æ—Ç—Ä—ñ–±–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ cloudscraper: pip install cloudscraper")
    raise



try:
    import openpyxl
    from openpyxl.styles import NamedStyle, Font, PatternFill, Border, Side, Alignment
    from openpyxl.utils.dataframe import dataframe_to_rows
except ImportError:
    print("[–ü–û–ú–ò–õ–ö–ê] –ü–æ—Ç—Ä—ñ–±–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ openpyxl: pip install openpyxl")
    raise

try:
    from bs4 import BeautifulSoup
    _HAVE_BS4 = True
except ImportError:
    _HAVE_BS4 = False


class RozetkaStockChecker:
    def __init__(self, debug=False, delay=0.7):
        self.scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False,
                'desktop': True
            },
            delay=10,
            interpreter='js2py'
        )
        self.base_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ru,en;q=0.9,en-GB;q=0.8,en-US;q=0.7,uk;q=0.6',
            'Content-Type': 'application/json',
            'Origin': 'https://rozetka.com.ua',
            'Referer': 'https://rozetka.com.ua/',
            'X-Requested-With': 'XMLHttpRequest',
        }
        self.debug = debug
        self.delay = delay
        self.reset_session_state()

    def reset_session_state(self):
        """–û—á–∏—â–∞—î–º–æ —Å—Ç–∞–Ω —Å–µ—Å—ñ—ó –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –Ω–æ–≤–æ–≥–æ —Ç–æ–≤–∞—Ä—É"""
        self.csrf_token = None
        self.purchase_id = None
        # –û—á–∏—â–∞—î–º–æ –∫—É–∫—ñ —Ç–∞ —Å—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π scraper –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É
        self.scraper = cloudscraper.create_scraper()

    def get_csrf_token(self):
        try:
            # Make request with additional headers to mimic a browser
            headers = self.base_headers.copy()
            headers.update({
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-Dest': 'document',
                'Upgrade-Insecure-Requests': '1'
            })
            resp = self.scraper.get('https://rozetka.com.ua/', headers=headers, timeout=10)
            resp.raise_for_status()

            # Check cookies
            cookies = self.scraper.cookies.get_dict()
            if self.debug:
                print("[–î–ï–ë–ê–ì] –í—Å–µ –∫—É–∫–∏:", cookies)

            possible_csrf_names = ['_uss-csrf', 'csrf-token', 'X-CSRF-TOKEN', 'csrf_token', '_token']
            for csrf_name in possible_csrf_names:
                if csrf_name in cookies:
                    self.csrf_token = cookies[csrf_name]
                    if self.debug:
                        print(f"[–î–ï–ë–ê–ì] –ù–∞–π–¥–µ–Ω CSRF —Ç–æ–∫–µ–Ω '{csrf_name}': {self.csrf_token}")
                    return True

            # Parse HTML for token
            html = resp.text
            csrf_patterns = [
                r'name="csrf-token"\s+content="([^"]+)"',
                r'"csrf_token"\s*:\s*"([^"]+)"',
                r'_uss-csrf["\']?\s*[:=]\s*["\']([^"\']+)',
                r'csrfToken["\']?\s*[:=]\s*["\']([^"\']+)',
                r'meta\[name=["\']?_?csrf[-_]?token["\']?\]\s*content=["\']([^"\']+)["\']'
            ]
            for pattern in csrf_patterns:
                match = re.search(pattern, html, re.I)
                if match:
                    self.csrf_token = match.group(1)
                    if self.debug:
                        print(f"[–î–ï–ë–ê–ì] CSRF —Ç–æ–∫–µ–Ω –Ω–∞–π–¥–µ–Ω –≤ HTML: {self.csrf_token}")
                    return True

            # Try API request to set cookies
            test_url = 'https://uss.rozetka.com.ua/session/cart-se/clear?country=UA&lang=ua'
            test_resp = self.scraper.post(test_url, json={}, headers=self.base_headers, timeout=10)
            cookies = self.scraper.cookies.get_dict()
            for csrf_name in possible_csrf_names:
                if csrf_name in cookies:
                    self.csrf_token = cookies[csrf_name]
                    if self.debug:
                        print(f"[–î–ï–ë–ê–ì] CSRF —Ç–æ–∫–µ–Ω –ø–æ–ª—É—á–µ–Ω –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {self.csrf_token}")
                    return True

            if self.debug:
                print("[–î–ï–ë–ê–ì] CSRF —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False

        except Exception as e:
            if self.debug:
                print(f"[CSRF] –ü–æ–º–∏–ª–∫–∞: {e}")
            return False
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –ø–æ–ø—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∫ API –±–µ–∑ —Ç–æ–∫–µ–Ω–∞
            # –ò–Ω–æ–≥–¥–∞ –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω—É–∂–Ω—ã–µ –∫—É–∫–∏
            try:
                test_url = 'https://uss.rozetka.com.ua/session/cart-se/clear?country=UA&lang=ua'
                test_resp = self.scraper.post(test_url, json={})
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—É–∫–∏ –µ—â–µ —Ä–∞–∑
                cookies = self.scraper.cookies.get_dict()
                for csrf_name in possible_csrf_names:
                    if csrf_name in cookies:
                        self.csrf_token = cookies[csrf_name]
                        if self.debug:
                            print(f"[–î–ï–ë–ê–ì] CSRF —Ç–æ–∫–µ–Ω –ø–æ–ª—É—á–µ–Ω –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {self.csrf_token}")
                        return True
            except:
                pass
            
            if self.debug:
                print("[–î–ï–ë–ê–ì] CSRF —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
            
        except Exception as e:
            print(f"[CSRF] –ü–æ–º–∏–ª–∫–∞: {e}")
            return False

    @staticmethod
    def extract_product_id(url: str):
        match = re.search(r'/p(\d+)/', url)
        return int(match.group(1)) if match else None

    def _ensure_csrf(self):
        if not self.csrf_token:
            if not self.get_csrf_token():
                raise RuntimeError("–ù–µ –≤–¥–∞–ª–æ—Å—å –æ—Ç—Ä–∏–º–∞—Ç–∏ CSRF —Ç–æ–∫–µ–Ω (_uss-csrf)")

    def clear_cart(self):
        """–û—á–∏—â–∞—î–º–æ –∫–æ—Ä–∑–∏–Ω—É –ø–µ—Ä–µ–¥ –¥–æ–¥–∞–≤–∞–Ω–Ω—è–º –Ω–æ–≤–æ–≥–æ —Ç–æ–≤–∞—Ä—É"""
        try:
            if not self.csrf_token:
                return
            
            url = 'https://uss.rozetka.com.ua/session/cart-se/clear?country=UA&lang=ua'
            headers = self.base_headers.copy()
            headers['CSRF-Token'] = self.csrf_token
            
            r = self.scraper.post(url, json={}, headers=headers)
            if self.debug:
                print("[–î–ï–ë–ê–ì] clear_cart —Å—Ç–∞—Ç—É—Å:", r.status_code)
                print("[–î–ï–ë–ê–ì] clear_cart —Ç—ñ–ª–æ:", r.text[:300])
        except Exception as e:
            if self.debug:
                print(f"[clear_cart] –ü–æ–º–∏–ª–∫–∞: {e}")

    def add_to_cart(self, product_id):
        self._ensure_csrf()
        
        # –û—á–∏—â–∞—î–º–æ –∫–æ—Ä–∑–∏–Ω—É –ø–µ—Ä–µ–¥ –¥–æ–¥–∞–≤–∞–Ω–Ω—è–º
        self.clear_cart()
        
        url = 'https://uss.rozetka.com.ua/session/cart-se/add?country=UA&lang=ua'
        headers = self.base_headers.copy()
        headers['CSRF-Token'] = self.csrf_token
        payload = [{"goods_id": product_id, "quantity": 1}]
        
        try:
            r = self.scraper.post(url, json=payload, headers=headers)
            if self.debug:
                print(f"[–î–ï–ë–ê–ì] add_to_cart —Å—Ç–∞—Ç—É—Å –¥–ª—è —Ç–æ–≤–∞—Ä—É {product_id}:", r.status_code)
                print("[–î–ï–ë–ê–ì] add_to_cart —Ç—ñ–ª–æ:", r.text[:500])
            
            if r.status_code == 200:
                data = r.json()
                goods_items = data.get('purchases', {}).get('goods')
                if goods_items and len(goods_items) > 0:
                    # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —â–æ —Ç–æ–≤–∞—Ä —Ç–æ–π —Å–∞–º–∏–π
                    for item in goods_items:
                        if item.get('goods', {}).get('id') == product_id:
                            self.purchase_id = item['id']
                            if self.debug:
                                print(f"[–î–ï–ë–ê–ì] purchase_id –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {self.purchase_id}")
                            return data
                    
                    print(f"[–ü–û–ü–ï–†–ï–î–ñ–ï–ù–ù–Ø] –¢–æ–≤–∞—Ä {product_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –∫–æ—Ä–∑–∏–Ω—ñ")
                    return None
                else:
                    print("[–ü–û–ü–ï–†–ï–î–ñ–ï–ù–ù–Ø] –ü–æ—Ä–æ–∂–Ω—è –∫–æ—Ä–∑–∏–Ω–∞ –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è")
                    return None
            return None
        except Exception as e:
            print(f"[add_to_cart] –ü–æ–º–∏–ª–∫–∞ –¥–ª—è —Ç–æ–≤–∞—Ä—É {product_id}: {e}")
            return None

    def update_quantity(self, quantity):
        if not self.purchase_id or not self.csrf_token:
            if self.debug:
                print(f"[update_quantity] –í—ñ–¥—Å—É—Ç–Ω—ñ –¥–∞–Ω—ñ: purchase_id={self.purchase_id}, csrf_token={bool(self.csrf_token)}")
            return None
            
        url = 'https://uss.rozetka.com.ua/session/cart-se/edit-quantity?country=UA&lang=ua'
        headers = self.base_headers.copy()
        headers['CSRF-Token'] = self.csrf_token
        payload = [{"purchase_id": self.purchase_id, "quantity": quantity}]
        
        try:
            r = self.scraper.post(url, json=payload, headers=headers)
            if self.debug:
                print(f"[–î–ï–ë–ê–ì] update_quantity({quantity}) —Å—Ç–∞—Ç—É—Å:", r.status_code)
                print("[–î–ï–ë–ê–ì] update_quantity —Ç—ñ–ª–æ:", r.text[:500])
            if r.status_code == 200:
                return r.json()
            return None
        except Exception as e:
            print(f"[update_quantity] –ü–æ–º–∏–ª–∫–∞: {e}")
            return None

    def binary_search_max_stock(self, product_id, max_attempts=100, upper_bound=10000):
        if self.debug:
            print(f"[–ë–ü] –ü–æ—á–∏–Ω–∞—î–º–æ –±—ñ–Ω–∞—Ä–Ω–∏–π –ø–æ—à—É–∫ –¥–ª—è —Ç–æ–≤–∞—Ä—É {product_id}")
        
        add_data = self.add_to_cart(product_id)
        if not add_data:
            print(f"[–ë–ü] –ù–µ –≤–¥–∞–ª–æ—Å—è –¥–æ–¥–∞—Ç–∏ —Ç–æ–≤–∞—Ä {product_id} –¥–æ –∫–æ—Ä–∑–∏–Ω–∏")
            return None, None

        left, right = 1, upper_bound
        max_available = 0

        for attempt in range(max_attempts):
            if left > right:
                break
                
            mid = (left + right) // 2
            if self.debug:
                print(f"[–ë–ü] #{attempt+1} —Ç–æ–≤–∞—Ä {product_id} -> —Ç–µ—Å—Ç—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å {mid}")
                
            data = self.update_quantity(mid)
            if not data:
                if self.debug:
                    print(f"[–ë–ü] –ù–µ –æ—Ç—Ä–∏–º–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ {mid}")
                break
                
            time.sleep(self.delay)
            
            errors = data.get('error_messages') or []
            not_enough = False
            
            for err in errors:
                if self.debug:
                    print(f"[–ë–ü] –ü–æ–º–∏–ª–∫–∞: {err}")
                if err.get('code') == 3002:  # –ö–æ–¥ –ø–æ–º–∏–ª–∫–∏ "–Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Ç–æ–≤–∞—Ä—É"
                    not_enough = True
                    break
                    
            if not_enough:
                right = mid - 1
                if self.debug:
                    print(f"[–ë–ü] –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Ç–æ–≤–∞—Ä—É –Ω–∞ {mid}, –∑–º–µ–Ω—à—É—î–º–æ –ø—Ä–∞–≤—É –º–µ–∂—É –¥–æ {right}")
            else:
                max_available = mid
                left = mid + 1
                if self.debug:
                    print(f"[–ë–ü] {mid} —Ç–æ–≤–∞—Ä—ñ–≤ –¥–æ—Å—Ç—É–ø–Ω–æ, –∑–±—ñ–ª—å—à—É—î–º–æ –ª—ñ–≤—É –º–µ–∂—É –¥–æ {left}")

        if self.debug:
            print(f"[–ë–ü] –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Ç–æ–≤–∞—Ä—É {product_id}: {max_available}")
            
        return max_available, add_data

    def parse_category_from_html(self, product_url, category_id):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –ø–∞—Ä—Å–∏–Ω–≥—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑ –±—ñ–ª—å—à —Ç–æ—á–Ω–∏–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏"""
        try:
            resp = self.scraper.get(product_url, timeout=15)
            html = resp.text
            
            if self.debug:
                print(f"[parse_category] –®—É–∫–∞—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é ID: {category_id}")
            
            # –°–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ –≤ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            json_patterns = [
                rf'"name"\s*:\s*"([^"]*)"[^}}]*"@id"\s*:\s*"[^"]*/{category_id}/',
                rf'"@id"\s*:\s*"[^"]*/{category_id}/[^"]*"[^}}]*"name"\s*:\s*"([^"]*)"',
                rf'"categoryId"\s*:\s*{category_id}[^}}]*"name"\s*:\s*"([^"]*)"',
                rf'"id"\s*:\s*{category_id}[^}}]*"title"\s*:\s*"([^"]*)"'
            ]
            
            for pattern in json_patterns:
                matches = re.finditer(pattern, html, re.I | re.S)
                for match in matches:
                    category_name = match.group(1).strip()
                    if category_name and len(category_name) > 2 and len(category_name) < 100:
                        if self.debug:
                            print(f"[parse_category] –ó–Ω–∞–π–¥–µ–Ω–æ –≤ JSON: '{category_name}'")
                        return category_name
            
            if _HAVE_BS4:
                soup = BeautifulSoup(html, 'html.parser')
                
                # –®—É–∫–∞—î–º–æ —É breadcrumbs (–Ω–∞–π–±—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω–æ)
                breadcrumb_selectors = [
                    '.breadcrumbs a',
                    '.rz-breadcrumbs a',
                    '[data-testid="breadcrumbs"] a',
                    '.catalog-heading a',
                    '.breadcrumb a',
                    'nav a',
                    '.rz-catalog-breadcrumbs a',
                    'a[rzrelnofollow].black-link',  # –î–æ–±–∞–≤–ª–µ–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è Angular-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                    'a[href*="/c"]'
                ]
                
                for selector in breadcrumb_selectors:
                    try:
                        elements = soup.select(selector)
                        for element in elements:
                            href = element.get('href', '')
                            # –®—É–∫–∞—î–º–æ —Ç–æ—á–Ω–∏–π –∑–±—ñ–≥ category_id –≤ URL
                            if re.search(rf'/c{category_id}(?:/|$)', href):
                                text = element.get_text(strip=True)
                                if text and len(text) > 2 and len(text) < 100:
                                    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
                                    if not any(skip in text.lower() for skip in ['>', '<', 'img', 'svg', 'icon', 'span']):
                                        if self.debug:
                                            print(f"[parse_category] –ó–Ω–∞–π–¥–µ–Ω–æ –≤ breadcrumbs: '{text}'")
                                        return text
                    except Exception as e:
                        if self.debug:
                            print(f"[parse_category] –ü–æ–º–∏–ª–∫–∞ breadcrumb —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ {selector}: {e}")
                        continue
                
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
                category_selectors = [
                    f'a[href*="/c{category_id}/"]',
                    f'a[href*="/ua/c{category_id}/"]',
                    f'*[data-category-id="{category_id}"]',
                    f'*[data-id="{category_id}"]'
                ]
                
                for selector in category_selectors:
                    try:
                        elements = soup.select(selector)
                        for element in elements:
                            text = element.get_text(strip=True)
                            if text and len(text) > 2 and len(text) < 100:
                                if not any(skip in text.lower() for skip in ['>', '<', 'function', 'script', 'style']):
                                    if self.debug:
                                        print(f"[parse_category] –ó–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º '{selector}': '{text}'")
                                    return text
                    except Exception as e:
                        if self.debug:
                            print(f"[parse_category] –ü–æ–º–∏–ª–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ {selector}: {e}")
                        continue
            
            # Regex –ø–æ—à—É–∫ —è–∫ –æ—Å—Ç–∞–Ω–Ω—ñ–π –≤–∞—Ä—ñ–∞–Ω—Ç
            regex_patterns = [
                rf'<a[^>]+href="[^"]*/?c{category_id}/[^"]*"[^>]*>([^<]+)</a>',
                rf'<a[^>]+href="[^"]*c{category_id}[^"]*"[^>]*>([^<]*?)</a>',
                rf'"name"\s*:\s*"([^"]*)"[^}}]*"categoryId"\s*:\s*"?{category_id}"?',
                rf'"title"\s*:\s*"([^"]*)"[^}}]*"id"\s*:\s*"?{category_id}"?'
            ]
            
            for pattern in regex_patterns:
                try:
                    matches = re.finditer(pattern, html, re.I | re.S)
                    for match in matches:
                        text = re.sub(r'<[^>]+>', '', match.group(1)).strip()
                        text = re.sub(r'\s+', ' ', text)
                        if text and len(text) > 2 and len(text) < 100:
                            if not any(skip in text.lower() for skip in ['function', 'script', 'style', '{', '}']): 
                                if self.debug:
                                    print(f"[parse_category] –ó–Ω–∞–π–¥–µ–Ω–æ regex: '{text}'")
                                return text
                except Exception as e:
                    if self.debug:
                        print(f"[parse_category] –ü–æ–º–∏–ª–∫–∞ pattern: {e}")
                    continue
            
            if self.debug:
                print(f"[parse_category] –ö–∞—Ç–µ–≥–æ—Ä—ñ—é –∑ ID {category_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                
        except Exception as e:
            if self.debug:
                print(f"[parse_category] –ó–∞–≥–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        
        return None

    def get_product_meta(self, product_url, add_data, product_id):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —Ç–æ–≤–∞—Ä—É"""
        title = None
        category_id = None
        category_name = None
        original_url = product_url
        
        # –°–ø–æ—á–∞—Ç–∫—É –Ω–∞–º–∞–≥–∞—î–º–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞–Ω—ñ –∑ API –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∫–æ—Ä–∑–∏–Ω–∏
        if add_data:
            goods_items = add_data.get('purchases', {}).get('goods')
            if goods_items:
                for item in goods_items:
                    goods = item.get('goods', {})
                    if goods.get('id') == product_id:
                        title = goods.get('title') or goods.get('name') or None
                        category_id = goods.get('category_id') or None
                        
                        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                        if title:
                            title = re.sub(r'\s+', ' ', title).strip()
                        
                        # –û–Ω–æ–≤–ª—é—î–º–æ URL —è–∫—â–æ —î –∫—Ä–∞—â–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
                        api_url = goods.get('href') or goods.get('url')
                        if api_url and api_url.startswith('http'):
                            product_url = api_url
                        break
        
        # –Ø–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑ API, –ø—Ä–æ–±—É—î–º–æ –ø–∞—Ä—Å–∏–Ω–≥ HTML
        if not title or not category_id:
            try:
                resp = self.scraper.get(original_url, timeout=15)
                html = resp.text
                
                # –ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞
                if not title and _HAVE_BS4:
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    title_selectors = [
                        'h1.product__title',
                        'h1[data-testid="product-title"]',
                        '.product-title h1',
                        'h1.rz-product-title',
                        '.product__title',
                        '[data-testid="product-title"]',
                        'h1'
                    ]
                    
                    for selector in title_selectors:
                        try:
                            element = soup.select_one(selector)
                            if element:
                                title = element.get_text(strip=True)
                                if title and len(title) > 5:
                                    title = re.sub(r'\s+', ' ', title).strip()
                                    break
                        except:
                            continue
                
                # –ü–æ–∏—Å–∫ category_id –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ API
                if not category_id:
                    # –®—É–∫–∞—î–º–æ –≤ –ø–æ—Ç–æ—á–Ω–æ–º—É URL
                    for url_to_check in [product_url, original_url]:
                        match = re.search(r'/c(\d+)/', url_to_check)
                        if match:
                            category_id = int(match.group(1))
                            break
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ HTML
                    if not category_id:
                        category_patterns = [
                            r'"category_id"\s*:\s*(\d+)',
                            r'"categoryId"\s*:\s*(\d+)',
                            r'data-category-id="(\d+)"',
                            r'category[_-]?id["\']?\s*[:=]\s*["\']?(\d+)'
                        ]
                        
                        for pattern in category_patterns:
                            match = re.search(pattern, html, re.I)
                            if match:
                                try:
                                    category_id = int(match.group(1))
                                    break
                                except:
                                    continue
                    
            except Exception as e:
                if self.debug:
                    print(f"[get_product_meta] –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É HTML: {e}")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–∞–∑–≤—É –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –±—ñ–ª—å—à –¥–µ—Ç–∞–ª—å–Ω–æ
        if category_id is not None:
            # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑ breadcrumbs
            category_name = self.get_category_from_breadcrumbs(product_url, category_id)
            
            # –Ø–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è, –ø—Ä–æ–±—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥
            if not category_name:
                category_name = self.parse_category_from_html(product_url, category_id)
            
            # –Ø–∫—â–æ –≤—Å–µ —â–µ –Ω–µ –≤–¥–∞–ª–æ—Å—è, –ø—Ä–æ–±—É—î–º–æ API Rozetka
            if not category_name:
                category_name = self.get_category_from_api(category_id)
                
            # –û—Å—Ç–∞–Ω–Ω—ñ–π fallback
            if not category_name:
                category_name = f"–ö–∞—Ç–µ–≥–æ—Ä—ñ—è #{category_id}"
        
        # –û—Å—Ç–∞–Ω–Ω—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–∏—Ö
        if title:
            title = title[:200]
        if category_name:
            category_name = category_name[:100]
            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —Å–∏–º–≤–æ–ª–∏
            category_name = re.sub(r'[<>{}"\']', '', category_name).strip()
            
        if self.debug:
            print(f"[get_product_meta] –†–µ–∑—É–ª—å—Ç–∞—Ç: title='{title}', category='{category_name}', category_id={category_id}")
            
        return title, category_name

    def get_category_from_breadcrumbs(self, product_url, category_id):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑ breadcrumbs –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
        try:
            resp = self.scraper.get(product_url, timeout=15)
            html = resp.text
            
            if self.debug:
                print(f"[breadcrumbs] –®—É–∫–∞—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é ID: {category_id}")
            
            if _HAVE_BS4:
                soup = BeautifulSoup(html, 'html.parser')
                
                # –†–æ–∑—à–∏—Ä–µ–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è breadcrumbs
                breadcrumb_selectors = [
                    '.breadcrumbs a',
                    '.rz-breadcrumbs a',
                    '[data-testid="breadcrumbs"] a',
                    '.catalog-heading a',
                    '.breadcrumb a',
                    'nav a',
                    '.rz-catalog-breadcrumbs a',
                    'a[rzrelnofollow].black-link',  # –î–æ–±–∞–≤–ª–µ–Ω —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è Angular-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                    'a[href*="/c"]'
                ]
                
                for selector in breadcrumb_selectors:
                    try:
                        links = soup.select(selector)
                        for link in links:
                            href = link.get('href', '')
                            
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –º—ñ—Å—Ç–∏—Ç—å –Ω–∞—à category_id
                            if f'/c{category_id}/' in href or f'c{category_id}' in href:
                                # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç, –æ—á–∏—â–∞—é—á–∏ –≤—ñ–¥ SVG —Ç–∞ —ñ–Ω—à–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
                                text_content = []
                                
                                # –ü—Ä–æ—Ö–æ–¥–∏–º–æ –ø–æ –≤—Å—ñ—Ö —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –≤—É–∑–ª–∞—Ö
                                for text_node in link.find_all(text=True):
                                    text = text_node.strip()
                                    # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ —Ä—è–¥–∫–∏ —Ç–∞ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
                                    if text and not text.startswith('icon-') and len(text) > 2:
                                        text_content.append(text)
                                
                                # –û–±'—î–¥–Ω—É—î–º–æ —Ç–µ–∫—Å—Ç
                                full_text = ' '.join(text_content).strip()
                                
                                # –û—á–∏—â–∞—î–º–æ –≤—ñ–¥ –∑–∞–π–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤
                                full_text = re.sub(r'\s+', ' ', full_text)
                                full_text = re.sub(r'^[^\w]*|[^\w]*$', '', full_text)  # –í–∏–¥–∞–ª—è—î–º–æ —Å–∏–º–≤–æ–ª–∏ –Ω–∞ –ø–æ—á–∞—Ç–∫—É/–∫—ñ–Ω—Ü—ñ
                                
                                if full_text and len(full_text) > 2 and len(full_text) < 100:
                                    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —â–æ —Ü–µ –Ω–µ —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π —Ç–µ–∫—Å—Ç
                                    if not any(skip in full_text.lower() for skip in [
                                        'svg', 'icon', 'chevron', 'use', 'href', 'assets', 'sprite'
                                    ]):
                                        if self.debug:
                                            print(f"[breadcrumbs] –ó–Ω–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é: '{full_text}' –≤ {href}")
                                        return full_text
                                
                    except Exception as e:
                        if self.debug:
                            print(f"[breadcrumbs] –ü–æ–º–∏–ª–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ {selector}: {e}")
                        continue
            
            # –Ø–∫—â–æ BeautifulSoup –Ω–µ –¥–æ–ø–æ–º—ñ–≥, –ø—Ä–æ–±—É—î–º–æ regex
            regex_patterns = [
                # –î–ª—è Angular –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –∑ rzrelnofollow
                rf'<a[^>]*rzrelnofollow[^>]*href="[^"]*c{category_id}[^"]*"[^>]*>(?:(?!</a>).)*?([^<>{{}}]+?)(?:(?!</a>).)*?</a>',
                # –ó–≤–∏—á–∞–π–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                rf'<a[^>]*href="[^"]*c{category_id}[^"]*"[^>]*>([^<]+)</a>',
                # –ë—ñ–ª—å—à —Å–∫–ª–∞–¥–Ω—ñ –≤–∏–ø–∞–¥–∫–∏ –∑ –≤–∫–ª–∞–¥–µ–Ω–∏–º–∏ —Ç–µ–≥–∞–º–∏
                rf'<a[^>]*href="[^"]*c{category_id}[^"]*"[^>]*>(.*?)</a>',
            ]
            
            for pattern in regex_patterns:
                try:
                    matches = re.finditer(pattern, html, re.I | re.S)
                    for match in matches:
                        raw_text = match.group(1)
                        
                        # –í–∏–¥–∞–ª—è—î–º–æ HTML —Ç–µ–≥–∏
                        clean_text = re.sub(r'<[^>]+>', '', raw_text)
                        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏
                        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
                        # –í–∏–¥–∞–ª—è—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Å–∏–º–≤–æ–ª–∏ –Ω–∞ –ø–æ—á–∞—Ç–∫—É/–∫—ñ–Ω—Ü—ñ
                        clean_text = re.sub(r'^[^\w\u0400-\u04FF]*|[^\w\u0400-\u04FF]*$', '', clean_text)
                        
                        if clean_text and len(clean_text) > 2 and len(clean_text) < 100:
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —â–æ —Ü–µ –Ω–µ —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π —Ç–µ–∫—Å—Ç
                            if not any(skip in clean_text.lower() for skip in [
                                'svg', 'icon', 'chevron', 'use', 'href', 'assets', 'sprite', 'ng-', 'function'
                            ]):
                                if self.debug:
                                    print(f"[breadcrumbs] –ó–Ω–∞–π–¥–µ–Ω–æ regex: '{clean_text}'")
                                return clean_text
                                
                except Exception as e:
                    if self.debug:
                        print(f"[breadcrumbs] –ü–æ–º–∏–ª–∫–∞ regex: {e}")
                    continue
            
            if self.debug:
                print(f"[breadcrumbs] –ö–∞—Ç–µ–≥–æ—Ä—ñ—é –¥–ª—è ID {category_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                
        except Exception as e:
            if self.debug:
                print(f"[breadcrumbs] –ó–∞–≥–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        
        return None

    def get_category_from_api(self, category_id):
        """–°–ø—Ä–æ–±–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é —á–µ—Ä–µ–∑ API Rozetka"""
        try:
            api_url = f"https://common-api.rozetka.com.ua/v2/fat-menu/full?country=UA&lang=ua"
            resp = self.scraper.get(api_url, timeout=10)
            
            if resp.status_code == 200:
                data = resp.json()
                
                def find_category_recursive(items, target_id):
                    for item in items:
                        if item.get('id') == target_id:
                            return item.get('title', item.get('name', ''))
                        
                        children = item.get('children', [])
                        if children:
                            result = find_category_recursive(children, target_id)
                            if result:
                                return result
                    return None
                
                result = find_category_recursive(data.get('data', []), category_id)
                if result:
                    return result
                    
        except Exception as e:
            if self.debug:
                print(f"[get_category_from_api] –ü–æ–º–∏–ª–∫–∞: {e}")
        
        return None

    def check_product(self, product_url):
        # –í–ê–ñ–õ–ò–í–û: –û—á–∏—â–∞—î–º–æ —Å—Ç–∞–Ω –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –∫–æ–∂–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É
        self.reset_session_state()
        
        product_id = self.extract_product_id(product_url)
        if not product_id:
            return {"error": "–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥—Ç–∏ ID", "url": product_url}

        print(f"=== –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–æ–≤–∞—Ä ID {product_id}: {product_url}")
        
        max_stock, add_data = self.binary_search_max_stock(product_id)
        if max_stock is None:
            return {"error": "–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å", "url": product_url, "product_id": product_id}

        title, category_name = self.get_product_meta(product_url, add_data, product_id)
        
        result = {
            "product_id": product_id,
            "url": product_url,
            "title": title or '',
            "category": category_name or '',
            "max_stock": max_stock,
        }
        
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Ç–æ–≤–∞—Ä—É {product_id}: {max_stock} —à—Ç. | {title or '–ë–µ–∑ –Ω–∞–∑–≤–∏'}")
        return result


# –†–µ—à—Ç–∞ –∫–æ–¥—É –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –±–µ–∑ –∑–º—ñ–Ω...
EXCEL_FILENAME = "rozetka_stock_history.xlsx"
EXCEL_FIELDS = ["name", "url", "category", "last_checked", "max_stock"]


def load_existing_excel(path: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π"""
    if not os.path.exists(path):
        return []
    
    try:
        from openpyxl import load_workbook
        workbook = load_workbook(path, read_only=True)
        worksheet = workbook.active
        
        # –ß–∏—Ç–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
        headers = []
        for cell in worksheet[1]:
            if cell.value:
                headers.append(cell.value)
            else:
                break
        
        if not headers:
            return []
        
        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = []
        for row in worksheet.iter_rows(min_row=2, values_only=True):
            if not any(row):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                continue
            
            row_dict = {}
            for i, value in enumerate(row):
                if i < len(headers):
                    row_dict[headers[i]] = value if value is not None else ''
                    
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–æ–ª—è
            for field in EXCEL_FIELDS:
                if field not in row_dict:
                    row_dict[field] = ''
                    
            data.append(row_dict)
        
        workbook.close()
        return data
        
    except Exception as e:
        print(f"[–ü–û–ü–ï–†–ï–î–ñ–ï–ù–ù–Ø] –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —ñ—Å–Ω—É—é—á–∏–π Excel —Ñ–∞–π–ª: {e}")
        print("–°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π —Ñ–∞–π–ª...")
        return []



def save_excel_with_formatting(path: str, data_list):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤ Excel —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not data_list:
        print("[–ü–û–ü–ï–†–ï–î–ñ–ï–ù–ù–Ø] –°–ø–∏—Å–æ–∫ –¥–∞–Ω–∏—Ö –ø–æ—Ä–æ–∂–Ω—ñ–π, —Å—Ç–≤–æ—Ä—é—î–º–æ —Ñ–∞–π–ª —Ç—ñ–ª—å–∫–∏ –∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏")
        data_list = []
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–æ–≤–∞—Ä–∞–º –∏ —Å–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
    products_history = {}
    all_dates = set()
    
    for row in data_list:
        url = row.get('url', '')
        if url not in products_history:
            products_history[url] = {
                'name': row.get('name', ''),
                'category': row.get('category', ''),
                'url': url,
                'dates': {}
            }
        
        date = row.get('last_checked', '')
        if date:
            date_only = date.split(' ')[0] if ' ' in date else date
            products_history[url]['dates'][date_only] = row.get('max_stock', 0)
            all_dates.add(date_only)
    
    sorted_dates = sorted(list(all_dates))
    
    from openpyxl import Workbook
    wb = Workbook()
    ws = wb.active
    ws.title = "–Ü—Å—Ç–æ—Ä–∏—è –∑–∞–ª–∏—à–∫—ñ–≤"

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    headers = ["–ù–∞–∑–≤–∞", "URL", "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è"] + sorted_dates
    for col_num, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col_num, value=header)
        
        cell.font = Font(name='Arial', size=12, bold=True, color='FFFFFF')
        cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
        cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
        cell.border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )

    # –î–∞–Ω–Ω—ã–µ
    row_num = 2
    for product_data in products_history.values():
        # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
        cell = ws.cell(row=row_num, column=1, value=product_data['name'])
        cell.font = Font(name='Arial', size=11)
        cell.alignment = Alignment(horizontal='left', vertical='center', wrap_text=True)
        cell.border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
        
        # URL
        cell = ws.cell(row=row_num, column=2, value=product_data['url'])
        cell.font = Font(name='Arial', size=11)
        cell.alignment = Alignment(horizontal='left', vertical='center', wrap_text=True)
        cell.border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
        cell = ws.cell(row=row_num, column=3, value=product_data['category'])
        cell.font = Font(name='Arial', size=11)
        cell.alignment = Alignment(horizontal='left', vertical='center', wrap_text=True)
        cell.border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
        
        # –î–∞–Ω–Ω—ã–µ –ø–æ –¥–∞—Ç–∞–º
        for col_idx, date in enumerate(sorted_dates, 4):
            stock_value = product_data['dates'].get(date, '')
            cell = ws.cell(row=row_num, column=col_idx, value=stock_value)
            
            cell.font = Font(name='Arial', size=11)
            cell.alignment = Alignment(horizontal='center', vertical='center')
            cell.border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
            
            # –¶–≤–µ—Ç–æ–≤–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if stock_value and stock_value > 0:
                cell.fill = PatternFill(start_color='C6EFCE', end_color='C6EFCE', fill_type='solid')
            elif stock_value == 0:
                cell.fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid')
            
            # –ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫
            if row_num % 2 == 0:
                if not cell.fill.start_color or cell.fill.start_color.rgb == '00000000':
                    cell.fill = PatternFill(start_color='F2F2F2', end_color='F2F2F2', fill_type='solid')
        
        row_num += 1

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —à–∏—Ä–∏–Ω—ã —Å—Ç–æ–ª–±—Ü–æ–≤
    ws.column_dimensions['A'].width = 40  # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
    ws.column_dimensions['B'].width = 60  # URL
    ws.column_dimensions['C'].width = 25  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
    
    # –î–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –¥–∞—Ç–∞–º–∏
    for col_idx in range(4, len(headers) + 1):
        col_letter = openpyxl.utils.get_column_letter(col_idx)
        ws.column_dimensions[col_letter].width = 12

    # –í—ã—Å–æ—Ç–∞ —Å—Ç—Ä–æ–∫
    for row in ws.iter_rows():
        ws.row_dimensions[row[0].row].height = 25

    # –ó–∞–∫—Ä–µ–ø–ª—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
    ws.freeze_panes = 'A2'
    
    # –ê–≤—Ç–æ—Ñ–∏–ª—å—Ç—Ä
    max_row = len(products_history) + 1
    max_col = len(headers)
    ws.auto_filter.ref = f"A1:{openpyxl.utils.get_column_letter(max_col)}{max_row}"

    try:
        wb.save(path)
    except Exception as e:
        print(f"[–û–®–ò–ë–ö–ê] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å Excel —Ñ–∞–π–ª: {e}")
        raise

def upsert_rows(existing_data, new_items):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏"""
    now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    new_rows = []
    
    for item in new_items:
        if 'error' in item:
            print(f"[–ü–û–ü–ï–†–ï–î–ñ–ï–ù–ù–Ø] {item.get('url', '–ù–µ–≤—ñ–¥–æ–º–∏–π URL')}: {item['error']}")
            continue
        new_rows.append({
            'name': item.get('title', ''),
            'url': item.get('url', ''),
            'category': item.get('category', ''),
            'last_checked': now_str,
            'max_stock': item.get('max_stock', 0),
        })
    
    if not existing_data:
        existing_data = []
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è —Ç–µ—Ö –∂–µ URL
    new_urls = [row['url'] for row in new_rows]
    filtered_existing = [row for row in existing_data if row.get('url', '') not in new_urls]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
    filtered_existing.extend(new_rows)
    
    return filtered_existing


def read_urls_from_file(fname):
    urls = []
    with open(fname, encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            urls.append(line)
    return urls


def get_interactive_urls():
    print("\n" + "="*70)
    print("üõí ROZETKA STOCK CHECKER - –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º")
    print("="*70)
    print("üìù –í–≤–µ–¥—ñ—Ç—å URL —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–∞–ª–∏—à–∫—ñ–≤:")
    print("   ‚Ä¢ –í–≤–æ–¥—å—Ç–µ –ø–æ –æ–¥–Ω–æ–º—É URL –≤ —Ä—è–¥–∫—É")
    print("   ‚Ä¢ –î–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –Ω–∞—Ç–∏—Å–Ω—ñ—Ç—å Enter –Ω–∞ –ø–æ—Ä–æ–∂–Ω—å–æ–º—É —Ä—è–¥–∫—É")
    print("   ‚Ä¢ –î–ª—è –≤–∏—Ö–æ–¥—É –≤–≤–µ–¥—ñ—Ç—å 'exit' –∞–±–æ 'quit'")
    print("-"*70)
    
    urls = []
    counter = 1
    
    while True:
        try:
            url = input(f"üîó URL ‚Ññ{counter}: ").strip()
            
            if not url:
                if urls:
                    print(f"\n‚úÖ –í–≤–µ–¥–µ–Ω–æ {len(urls)} URL(s). –ü–æ—á–∏–Ω–∞—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É...")
                    break
                else:
                    print("‚ùå –ù–µ –≤–≤–µ–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ URL. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑.")
                    continue
            
            if url.lower() in ['exit', 'quit', '–≤–∏—Ö—ñ–¥']:
                print("üëã –í–∏—Ö—ñ–¥ –∑ –ø—Ä–æ–≥—Ä–∞–º–∏...")
                sys.exit(0)
            
            if url.startswith('http') and 'rozetka.com.ua' in url:
                urls.append(url)
                print(f"   ‚úì URL ‚Ññ{counter} –¥–æ–¥–∞–Ω–æ")
                counter += 1
            else:
                print("   ‚ùå URL –º–∞—î –ø–æ—á–∏–Ω–∞—Ç–∏—Å—è –∑ http:// –∞–±–æ https:// —Ç–∞ –º—ñ—Å—Ç–∏—Ç–∏ rozetka.com.ua")
                
        except KeyboardInterrupt:
            print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–∞ –ø–µ—Ä–µ—Ä–≤–∞–Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
            sys.exit(0)
    
    return urls

def get_interactive_urls():
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω—ã –ª–∏ –º—ã –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
    if not sys.stdin.isatty():
        print("‚ùå –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ –Ω–µ–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ä–µ–¥–µ")
        print("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ —Ñ–∞–π–ª —Å URL")
        return []
    
    print("\n" + "="*70)
    print("üõí ROZETKA STOCK CHECKER - –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º")
    print("="*70)
    print("üìù –í–≤–µ–¥—ñ—Ç—å URL —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–∞–ª–∏—à–∫—ñ–≤:")
    print("   ‚Ä¢ –í–≤–æ–¥—å—Ç–µ –ø–æ –æ–¥–Ω–æ–º—É URL –≤ —Ä—è–¥–∫—É")
    print("   ‚Ä¢ –î–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –Ω–∞—Ç–∏—Å–Ω—ñ—Ç—å Enter –Ω–∞ –ø–æ—Ä–æ–∂–Ω—å–æ–º—É —Ä—è–¥–∫—É")
    print("   ‚Ä¢ –î–ª—è –≤–∏—Ö–æ–¥—É –≤–≤–µ–¥—ñ—Ç—å 'exit' –∞–±–æ 'quit'")
    print("-"*70)
    
    urls = []
    counter = 1
    
    while True:
        try:
            url = input(f"üîó URL ‚Ññ{counter}: ").strip()
            
            if not url:
                if urls:
                    print(f"\n‚úÖ –í–≤–µ–¥–µ–Ω–æ {len(urls)} URL(s). –ü–æ—á–∏–Ω–∞—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É...")
                    break
                else:
                    print("‚ùå –ù–µ –≤–≤–µ–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ URL. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑.")
                    continue
            
            if url.lower() in ['exit', 'quit', '–≤–∏—Ö—ñ–¥']:
                print("üëã –í–∏—Ö—ñ–¥ –∑ –ø—Ä–æ–≥—Ä–∞–º–∏...")
                sys.exit(0)
            
            if url.startswith('http') and 'rozetka.com.ua' in url:
                urls.append(url)
                print(f"   ‚úì URL ‚Ññ{counter} –¥–æ–¥–∞–Ω–æ")
                counter += 1
            else:
                print("   ‚ùå URL –º–∞—î –ø–æ—á–∏–Ω–∞—Ç–∏—Å—è –∑ http:// –∞–±–æ https:// —Ç–∞ –º—ñ—Å—Ç–∏—Ç–∏ rozetka.com.ua")
                
        except KeyboardInterrupt:
            print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–∞ –ø–µ—Ä–µ—Ä–≤–∞–Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
            sys.exit(0)
        except EOFError:
            print("\n‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–≤–æ–¥—É. –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏.")
            break
    
    return urls

def parse_cli():
    p = argparse.ArgumentParser(description="Rozetka stock checker -> Excel —Ç–∞–±–ª–∏—Ü—è")
    p.add_argument('urls', nargs='*', help='URL —Ç–æ–≤–∞—Ä—ñ–≤')
    p.add_argument('-f', '--file', help='–§–∞–π–ª –∑—ñ —Å–ø–∏—Å–∫–æ–º URL (–ø–æ 1 –≤ —Ä—è–¥–∫—É)')
    p.add_argument('--interactive', action='store_true', help='–Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º –¥–ª—è –≤–≤–æ–¥—É URL')
    p.add_argument('--debug', action='store_true', help='–î–µ–±–∞–≥ –≤–∏–≤—ñ–¥')
    p.add_argument('--delay', type=float, default=0.7, help='–ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ –ø—ñ–¥ —á–∞—Å –±—ñ–Ω–∞—Ä–Ω–æ–≥–æ –ø–æ—à—É–∫—É')
    return p.parse_args()


def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ Rozetka Stock Checker...")
    
    args = parse_cli()
    urls = list(args.urls)
    
    if args.file:
        print(f"üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ URL –∑ —Ñ–∞–π–ª—É: {args.file}")
        urls.extend(read_urls_from_file(args.file))
    
    if not urls:
        print("üîÑ –ó–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º...")
        urls = get_interactive_urls()
    
    if not urls:
        print("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ URL –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏!")
        return

    print(f"\nüéØ –ó–Ω–∞–π–¥–µ–Ω–æ {len(urls)} —Ç–æ–≤–∞—Ä—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏")
    print("‚è≥ –ü–æ—á–∏–Ω–∞—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –∑–∞–ª–∏—à–∫—ñ–≤...\n")

    checker = RozetkaStockChecker(debug=args.debug, delay=args.delay)
    results = []
    
    for i, url in enumerate(urls, 1):
        print(f"[{i}/{len(urls)}] –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–æ–≤–∞—Ä...")
        res = checker.check_product(url)
        results.append(res)
        
        if i < len(urls):
            print("‚è±Ô∏è  –ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏...")
            time.sleep(2)  # –ó–±—ñ–ª—å—à–µ–Ω–æ –ø–∞—É–∑—É –º—ñ–∂ —Ç–æ–≤–∞—Ä–∞–º–∏

    existing = load_existing_excel(EXCEL_FILENAME)
    merged = upsert_rows(existing, results)
    
    save_excel_with_formatting(EXCEL_FILENAME, merged)

    print("\n" + "="*70)
    print("‚úÖ –ì–û–¢–û–í–û! –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏:")
    print("="*70)
    print(f"üìä –§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {os.path.abspath(EXCEL_FILENAME)}")
    print(f"üìà –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤ –≤ —Ç–∞–±–ª–∏—Ü—ñ: {len(merged)}")
    print("-"*70)
    

    success_count = 0
    for item in results:
        if 'error' in item:
            print(f"‚ùå –ü–û–ú–ò–õ–ö–ê: {item['url']} - {item['error']}")
        else:
            success_count += 1
            print(f"‚úÖ {item['title']}")
            print(f"   üìÇ –ö–∞—Ç–µ–≥–æ—Ä—ñ—è: {item['category']}")
            print(f"   üì¶ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å: {item['max_stock']}")
            print(f"   üîó URL: {item['url'][:60]}...")
            print()
    
    print("="*70)
    print(f"üéâ –£—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {success_count}/{len(results)} —Ç–æ–≤–∞—Ä—ñ–≤")


if __name__ == '__main__':
    main()
